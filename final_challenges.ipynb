{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anaferreira744/DE-DP-ADF/blob/main/final_challenges.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/lake"
      ],
      "metadata": {
        "id": "zr3Jgn4st0_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_GBE9UsyxwK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "a4361f5d-5505-4db9-f14e-4fda65481ba8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcybt71kTDNt"
      },
      "source": [
        "# Context\n",
        "Message events are coming from platform message broker (kafka, pubsub, kinesis...).\n",
        "You need to process the data according to the requirements.\n",
        "\n",
        "Message schema:\n",
        "- timestamp\n",
        "- value\n",
        "- event_type\n",
        "- message_id\n",
        "- country_id\n",
        "- user_id\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkyPORKNSYvV"
      },
      "source": [
        "# Challenge 1\n",
        "\n",
        "Step 1\n",
        "- Change exising producer\n",
        "\t- Change parquet location to \"/content/lake/bronze/messages/data\"\n",
        "\t- Add checkpoint (/content/lake/bronze/messages/checkpoint)\n",
        "\t- Delete /content/lake/bronze/messages and reprocess data\n",
        "\t- For reprocessing, run the streaming for at least 1 minute, then stop it\n",
        "\n",
        "Step 2\n",
        "- Implement new stream job to read from messages in bronze layer and split result in two locations\n",
        "\t- \"messages_corrupted\"\n",
        "\t\t- logic: event_status is null, empty or equal to \"NONE\"\n",
        "    - extra logic: add country name by joining message with countries dataset\n",
        "\t\t- partition by \"date\" -extract it from timestamp\n",
        "\t\t- location: /content/lake/silver/messages_corrupted/data\n",
        "\n",
        "\t- \"messages\"\n",
        "\t\t- logic: not corrupted data\n",
        "\t\t- extra logic: add country name by joining message with countries dataset\n",
        "\t\t- partition by \"date\" -extract it from timestamp\n",
        "\t\t- location: /content/lake/silver/messages/data\n",
        "\n",
        "\t- technical requirements\n",
        "\t\t- add checkpint (choose location)\n",
        "\t\t- use StructSchema\n",
        "\t\t- Set trigger interval to 5 seconds\n",
        "\t\t- run streaming for at least 20 seconds, then stop it\n",
        "\n",
        "\t- alternatives\n",
        "\t\t- implementing single streaming job with foreach/- foreachBatch logic to write into two locations\n",
        "\t\t- implementing two streaming jobs, one for messages and another for messages_corrupted\n",
        "\t\t- (paying attention on the paths and checkpoints)\n",
        "\n",
        "\n",
        "  - Check results:\n",
        "    - results from messages in bronze layer should match with the sum of messages+messages_corrupted in the silver layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Udk3tohSaXOH",
        "outputId": "0b617ed7-70a2-451f-c5d4-8fbc949e4215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.10/dist-packages (33.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from faker) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install faker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDGMKwBdi1qy"
      },
      "source": [
        "# Producer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPCOdivrfhYh",
        "outputId": "0cb66233-7654-4ece-b119-724700eff12f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "from faker import Faker\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('Test streaming').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "fake = Faker()\n",
        "messages = [fake.uuid4() for _ in range(50)]\n",
        "\n",
        "def enrich_data(df, messages=messages):\n",
        "  fake = Faker()\n",
        "  new_columns = {\n",
        "      'event_type': F.lit(fake.random_element(elements=('OPEN', 'RECEIVED', 'SENT', 'CREATED', 'CLICKED', '', 'NONE'))),\n",
        "      'message_id': F.lit(fake.random_element(elements=messages)),\n",
        "      'channel': F.lit(fake.random_element(elements=('CHAT', 'EMAIL', 'SMS', 'PUSH', 'OTHER'))),\n",
        "      'country_id': F.lit(fake.random_int(min=2000, max=2015)),\n",
        "      'user_id': F.lit(fake.random_int(min=1000, max=1050)),\n",
        "  }\n",
        "  df = df.withColumns(new_columns)\n",
        "  return df\n",
        "\n",
        "def insert_messages(df: DataFrame, batch_id):\n",
        "  enrich = enrich_data(df)\n",
        "  enrich.write.mode(\"append\").format(\"parquet\").save(\"/content/lake/bronze/messages/data\")\n",
        "\n",
        "# read stream\n",
        "df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
        "\n",
        "# write stream\n",
        "query = (df_stream.writeStream\n",
        ".outputMode('append')\n",
        ".trigger(processingTime='1 seconds')\n",
        ".option(\"checkpointLocation\", \"/content/lake/bronze/messages/checkpoint\")\n",
        ".foreachBatch(insert_messages)\n",
        ".start()\n",
        ")\n",
        "\n",
        "\n",
        "query.awaitTermination(60)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query.isActive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FjLpEzIF2niK",
        "outputId": "ed913108-d8dd-4db1-ec39-0b22754bd718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNyUK3yplDhg"
      },
      "outputs": [],
      "source": [
        "query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWQExsnzlMFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dd10adb-1862-4e01-feca-ecf669624bf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "|           timestamp|value|event_type|          message_id|channel|country_id|user_id|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "|2024-12-11 21:56:...|    6|  RECEIVED|897004f2-01ba-415...|  EMAIL|      2003|   1002|\n",
            "|2024-12-11 21:56:...|   49|  RECEIVED|8047a735-0310-403...|  OTHER|      2006|   1013|\n",
            "|2024-12-11 21:56:...|   11|  RECEIVED|1e381dd3-81b1-4f7...|   CHAT|      2000|   1034|\n",
            "|2024-12-11 21:57:...|   54|   CLICKED|8047a735-0310-403...|  EMAIL|      2008|   1038|\n",
            "|2024-12-11 21:56:...|   47|   CLICKED|024b470e-46af-4f7...|  EMAIL|      2006|   1018|\n",
            "|2024-12-11 21:56:...|   53|   CREATED|c3883efb-4464-4b6...|  OTHER|      2002|   1016|\n",
            "|2024-12-11 21:56:...|    0|   CREATED|0cf9eec4-6142-4d8...|  OTHER|      2000|   1015|\n",
            "|2024-12-11 21:57:...|   62|   CLICKED|1e381dd3-81b1-4f7...|  OTHER|      2001|   1015|\n",
            "|2024-12-11 21:56:...|   36|   CREATED|8ed7a293-df3c-463...|   PUSH|      2014|   1020|\n",
            "|2024-12-11 21:56:...|   21|  RECEIVED|96b38daf-81bc-482...|    SMS|      2000|   1039|\n",
            "|2024-12-11 21:56:...|   46|   CLICKED|3dde33fc-8622-401...|   CHAT|      2006|   1002|\n",
            "|2024-12-11 21:56:...|   23|   CLICKED|cfb5a0c2-7b76-4d1...|   CHAT|      2013|   1011|\n",
            "|2024-12-11 21:56:...|   12|   CLICKED|63671f2f-3b1c-4bd...|   CHAT|      2002|   1001|\n",
            "|2024-12-11 21:56:...|   18|   CLICKED|aacace93-42d6-413...|   CHAT|      2007|   1032|\n",
            "|2024-12-11 21:56:...|   20|  RECEIVED|63749773-efde-49e...|    SMS|      2013|   1019|\n",
            "|2024-12-11 21:57:...|   59|   CREATED|8b66ce15-e810-43b...|    SMS|      2003|   1017|\n",
            "|2024-12-11 21:56:...|   51|   CREATED|2fc5636e-72bb-482...|    SMS|      2014|   1047|\n",
            "|2024-12-11 21:56:...|   35|   CREATED|4fb2189d-0477-47d...|    SMS|      2007|   1021|\n",
            "|2024-12-11 21:56:...|   33|      OPEN|c3883efb-4464-4b6...|  EMAIL|      2010|   1009|\n",
            "|2024-12-11 21:56:...|   43|      NONE|ff4f60eb-1999-4d8...|  EMAIL|      2007|   1023|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "df = spark.read.format(\"parquet\").load(\"/content/lake/bronze/messages/data/*\")\n",
        "df.show()\n",
        "df.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RraxHCycMdEZ"
      },
      "source": [
        "# Additional datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfsus3dxMcQI"
      },
      "outputs": [],
      "source": [
        "countries = [\n",
        "    {\"country_id\": 2000, \"country\": \"Brazil\"},\n",
        "    {\"country_id\": 2001, \"country\": \"Portugal\"},\n",
        "    {\"country_id\": 2002, \"country\": \"Spain\"},\n",
        "    {\"country_id\": 2003, \"country\": \"Germany\"},\n",
        "    {\"country_id\": 2004, \"country\": \"France\"},\n",
        "    {\"country_id\": 2005, \"country\": \"Italy\"},\n",
        "    {\"country_id\": 2006, \"country\": \"United Kingdom\"},\n",
        "    {\"country_id\": 2007, \"country\": \"United States\"},\n",
        "    {\"country_id\": 2008, \"country\": \"Canada\"},\n",
        "    {\"country_id\": 2009, \"country\": \"Australia\"},\n",
        "    {\"country_id\": 2010, \"country\": \"Japan\"},\n",
        "    {\"country_id\": 2011, \"country\": \"China\"},\n",
        "    {\"country_id\": 2012, \"country\": \"India\"},\n",
        "    {\"country_id\": 2013, \"country\": \"South Korea\"},\n",
        "    {\"country_id\": 2014, \"country\": \"Russia\"},\n",
        "    {\"country_id\": 2015, \"country\": \"Argentina\"}\n",
        "]\n",
        "\n",
        "countries = spark.createDataFrame(countries)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pg2nx03_Sn62"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swvPj9hVpzNf"
      },
      "source": [
        "# Streaming Messages x Messages Corrupted"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "\n",
        "# Definição do schema explícito para os dados\n",
        "schema = StructType([\n",
        "    StructField(\"event_type\", StringType(), True),\n",
        "    StructField(\"message_id\", StringType(), True),\n",
        "    StructField(\"channel\", StringType(), True),\n",
        "    StructField(\"country_id\", IntegerType(), True),\n",
        "    StructField(\"user_id\", IntegerType(), True),\n",
        "    StructField(\"timestamp\", TimestampType(), True)\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "# Função para escrever os dados em dois caminhos diferentes (dados válidos e corrompidos)\n",
        "def split(df: DataFrame, batch_id):\n",
        "\n",
        "    df=df.join(countries, on=\"country_id\", how=\"left\")\n",
        "\n",
        "    # Dados corrompidos: event_type é null, vazio ou \"NONE\"\n",
        "    corrupted_df = df.filter(\n",
        "        (F.col(\"event_type\").isNull()) |\n",
        "        (F.col(\"event_type\") == \"\") |\n",
        "        (F.col(\"event_type\") == \"NONE\")\n",
        "    )\n",
        "\n",
        "    # Escreve os dados corrompidos\n",
        "    corrupted_df.write.mode(\"append\").format(\"parquet\") \\\n",
        "        .partitionBy(\"date\") \\\n",
        "        .save(\"/content/lake/silver/messages_corrupted/data\")\n",
        "\n",
        "    # Dados válidos: não corrompidos\n",
        "    valid_df = df.filter(~(\n",
        "        (F.col(\"event_type\").isNull()) |\n",
        "        (F.col(\"event_type\") == \"\") |\n",
        "        (F.col(\"event_type\") == \"NONE\")\n",
        "    ))\n",
        "\n",
        "    # Escreve os dados válidos\n",
        "    valid_df.write.mode(\"append\").format(\"parquet\") \\\n",
        "        .partitionBy(\"date\") \\\n",
        "        .save(\"/content/lake/silver/messages/data\")\n",
        "\n",
        "# Leitura do streaming de dados da camada Bronze\n",
        "df_stream = spark.readStream.format(\"parquet\") \\\n",
        "    .schema(schema) \\\n",
        "    .load(\"/content/lake/bronze/messages/data/*\")\n",
        "\n",
        "# Enriquecendo os dados com o nome do país e extraindo a data\n",
        "df_enriched = df_stream.withColumn(\"date\", F.to_date(\"timestamp\"))\n",
        "\n",
        "# Configuração do streaming para dividir os dados e escrever nos caminhos apropriados\n",
        "query = (df_enriched.writeStream\n",
        "    .outputMode(\"append\")\n",
        "    .trigger(processingTime=\"5 seconds\")\n",
        "    .option(\"checkpointLocation\", \"/content/lake/silver/checkpoint\")\n",
        "    .foreachBatch(split)\n",
        "    .start()\n",
        ")\n",
        "\n",
        "# Executar o streaming por pelo menos 20 segundos\n",
        "query.awaitTermination(20)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L33x8WRi6tkS",
        "outputId": "7a1db571-c675-4147-84b2-907d97f96273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()"
      ],
      "metadata": {
        "id": "gPvkCyJkUjLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLK9jpjCu3xE"
      },
      "source": [
        "## Checking data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nk8seEvbmvcU",
        "outputId": "88fadb2c-f201-45f1-983c-78e11eb32897",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The counts match: Bronze layer count is equal to the sum of valid and corrupted messages in the Silver layer.\n"
          ]
        }
      ],
      "source": [
        "# Count messages in Bronze Layer (messages in /content/lake/bronze/messages/data/*)\n",
        "bronze_df = spark.read.parquet(\"/content/lake/bronze/messages/data/*\")\n",
        "bronze_count = bronze_df.count()\n",
        "\n",
        "# Count messages in Silver Layer (valid messages in /content/lake/silver/messages/data and corrupted messages in /content/lake/silver/messages_corrupted/data)\n",
        "valid_messages_df = spark.read.parquet(\"/content/lake/silver/messages/data\")\n",
        "corrupted_messages_df = spark.read.parquet(\"/content/lake/silver/messages_corrupted/data\")\n",
        "\n",
        "valid_messages_count = valid_messages_df.count()\n",
        "corrupted_messages_count = corrupted_messages_df.count()\n",
        "\n",
        "# Calculate total messages in Silver Layer (valid + corrupted)\n",
        "silver_total_count = valid_messages_count + corrupted_messages_count\n",
        "\n",
        "# Perform the check\n",
        "if bronze_count == silver_total_count:\n",
        "    print(\"The counts match: Bronze layer count is equal to the sum of valid and corrupted messages in the Silver layer.\")\n",
        "else:\n",
        "    print(\"The counts do not match!\")\n",
        "    print(f\"Bronze Layer Count: {bronze_count}\")\n",
        "    print(f\"Silver Layer Count (Valid + Corrupted): {silver_total_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfxIlBISSvRP"
      },
      "source": [
        "# Challenge 2\n",
        "\n",
        "- Run business report\n",
        "- But first, there is a bug in the system which is causing some duplicated messages, we need to exclude these lines from the report\n",
        "\n",
        "- removing duplicates logic:\n",
        "  - Identify possible duplicates on message_id, event_type and channel\n",
        "  - in case of duplicates, consider only the first message (occurrence by timestamp)\n",
        "  - Ex:\n",
        "    In table below, the correct message to consider is the second line\n",
        "\n",
        "```\n",
        "    message_id | channel | event_type | timestamp\n",
        "    123        | CHAT    | CREATED    | 10:10:01\n",
        "    123        | CHAT    | CREATED    | 07:56:45 (first occurrence)\n",
        "    123        | CHAT    | CREATED    | 08:13:33\n",
        "```\n",
        "\n",
        "- After cleaning the data we're able to create the busines report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3J9XyOHhqvU"
      },
      "outputs": [],
      "source": [
        "# dedup data\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "df = spark.read.format(\"parquet\").load(\"/content/lake/silver/messages\")\n",
        "dedup = df.withColumn(\"row_number\", F.row_number().over(Window.partitionBy(\"message_id\", \"event_type\", \"channel\").orderBy(\"timestamp\"))).filter(\"row_number = 1\").drop(\"row_number\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF9L9i25lk74"
      },
      "source": [
        "### Report 1\n",
        "  - Aggregate data by date, event_type and channel\n",
        "  - Count number of messages\n",
        "  - pivot event_type from rows into columns\n",
        "  - schema expected:\n",
        "  \n",
        "```\n",
        "|      date|channel|CLICKED|CREATED|OPEN|RECEIVED|SENT|\n",
        "+----------+-------+-------+-------+----+--------+----+\n",
        "|2024-12-03|    SMS|      4|      4|   1|       1|   5|\n",
        "|2024-12-03|   CHAT|      3|      7|   5|       8|   4|\n",
        "|2024-12-03|   PUSH|   NULL|      3|   4|       3|   4|\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Agrupar por 'date', 'event_type', e 'channel' e contar o número de mensagens\n",
        "aggregated_df = df.groupBy(\"date\", \"event_type\", \"channel\").agg(\n",
        "    F.count(\"*\").alias(\"message_count\")\n",
        ")\n",
        "\n",
        "# 2. Pivotar a coluna 'event_type' para que cada tipo de evento se torne uma coluna\n",
        "pivoted_df = aggregated_df.groupBy(\"date\", \"channel\").pivot(\"event_type\").agg(\n",
        "    F.sum(\"message_count\").alias(\"message_count\")\n",
        ")\n",
        "\n",
        "# 3. Exibir os resultados\n",
        "pivoted_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVCybb74vAsV",
        "outputId": "0c2bb8e1-376c-44d6-8371-49a07fcaaf23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+-------+-------+----+--------+----+\n",
            "|      date|channel|CLICKED|CREATED|OPEN|RECEIVED|SENT|\n",
            "+----------+-------+-------+-------+----+--------+----+\n",
            "|2024-12-11|   PUSH|   NULL|      1|   1|    NULL|   3|\n",
            "|2024-12-11|    SMS|   NULL|      3|   1|       2|   1|\n",
            "|2024-12-11|  EMAIL|      2|   NULL|   4|       1|NULL|\n",
            "|2024-12-11|  OTHER|      1|      2|   3|       1|   4|\n",
            "|2024-12-11|   CHAT|      4|   NULL|NULL|       1|   5|\n",
            "+----------+-------+-------+-------+----+--------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPHSMSXnTKgu"
      },
      "outputs": [],
      "source": [
        "# report 1\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxwOawo2lwQH"
      },
      "source": [
        "## Report 2\n",
        "\n",
        "- Identify the most active users by channel (sorted by number of iterations)\n",
        "- schema expected:\n",
        "\n",
        "```\n",
        "+-------+----------+----+-----+-----+----+---+\n",
        "|user_id|iterations|CHAT|EMAIL|OTHER|PUSH|SMS|\n",
        "+-------+----------+----+-----+-----+----+---+\n",
        "|   1022|         5|   2|    0|    1|   0|  2|\n",
        "|   1004|         4|   1|    1|    1|   1|  0|\n",
        "|   1013|         4|   0|    0|    2|   1|  1|\n",
        "|   1020|         4|   2|    0|    1|   1|  0|\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# 1. Group by 'user_id' and 'channel', then count the number of messages per user for each channel\n",
        "user_activity_df = df.groupBy(\"user_id\", \"channel\").agg(\n",
        "    F.count(\"*\").alias(\"iterations\")\n",
        ")\n",
        "\n",
        "# 2. Pivot the data to have each channel as a separate column\n",
        "pivot_df = user_activity_df.groupBy(\"user_id\").pivot(\"channel\", [\"CHAT\", \"EMAIL\", \"OTHER\", \"PUSH\", \"SMS\"]).agg(\n",
        "    F.sum(\"iterations\").alias(\"iterations\")\n",
        ")\n",
        "\n",
        "# 3. Fill null values with 0 (as users may not have messages in some channels)\n",
        "pivot_df = pivot_df.fillna(0)\n",
        "\n",
        "# 4. Calculate the total iterations (sum of messages across all channels for each user)\n",
        "pivot_df = pivot_df.withColumn(\"total_iterations\",\n",
        "                               F.col(\"CHAT\") + F.col(\"EMAIL\") + F.col(\"OTHER\") + F.col(\"PUSH\") + F.col(\"SMS\"))\n",
        "\n",
        "# 5. Reorganize columns to place 'total_iterations' in the second position\n",
        "pivot_df = pivot_df.select(\n",
        "    \"user_id\", \"total_iterations\", \"CHAT\", \"EMAIL\", \"OTHER\", \"PUSH\", \"SMS\"\n",
        ")\n",
        "\n",
        "# 6. Sort by the total number of iterations in descending order\n",
        "sorted_df = pivot_df.orderBy(F.col(\"total_iterations\"), ascending=False)\n",
        "\n",
        "# 7. Show the results\n",
        "sorted_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOOo8LzPwQM2",
        "outputId": "8a8b10e7-a91b-4e5c-d931-02467ce8ecf5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----------------+----+-----+-----+----+---+\n",
            "|user_id|total_iterations|CHAT|EMAIL|OTHER|PUSH|SMS|\n",
            "+-------+----------------+----+-----+-----+----+---+\n",
            "|   1016|               2|   1|    0|    1|   0|  0|\n",
            "|   1034|               2|   1|    0|    1|   0|  0|\n",
            "|   1028|               2|   1|    1|    0|   0|  0|\n",
            "|   1032|               2|   1|    1|    0|   0|  0|\n",
            "|   1002|               2|   1|    1|    0|   0|  0|\n",
            "|   1015|               2|   0|    0|    2|   0|  0|\n",
            "|   1007|               2|   0|    0|    1|   0|  1|\n",
            "|   1049|               2|   0|    0|    0|   2|  0|\n",
            "|   1038|               2|   1|    1|    0|   0|  0|\n",
            "|   1013|               2|   0|    0|    2|   0|  0|\n",
            "|   1009|               2|   1|    1|    0|   0|  0|\n",
            "|   1031|               1|   0|    0|    1|   0|  0|\n",
            "|   1019|               1|   0|    0|    0|   0|  1|\n",
            "|   1046|               1|   0|    0|    0|   1|  0|\n",
            "|   1047|               1|   0|    0|    0|   0|  1|\n",
            "|   1021|               1|   0|    0|    0|   0|  1|\n",
            "|   1048|               1|   0|    0|    1|   0|  0|\n",
            "|   1017|               1|   0|    0|    0|   0|  1|\n",
            "|   1020|               1|   0|    0|    0|   1|  0|\n",
            "|   1001|               1|   1|    0|    0|   0|  0|\n",
            "+-------+----------------+----+-----+-----+----+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsS7bkAJmWsW"
      },
      "outputs": [],
      "source": [
        "# report 2\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9_kzDbDwDOS"
      },
      "source": [
        "# Challenge 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef0RjFTxwE5y"
      },
      "outputs": [],
      "source": [
        "# Theoretical question:\n",
        "\n",
        "# A new usecase requires the message data to be aggregate in near real time\n",
        "# They want to build a dashboard embedded in the platform website to analyze message data in low latency (few minutes)\n",
        "# This application will access directly the data aggregated by streaming process\n",
        "\n",
        "# Q1:\n",
        "- What would be your suggestion to achieve that using Spark Structure Streaming?\n",
        "Or would you choose a different data processing tool?\n",
        "\n",
        "- Which storage would you use and why? (database?, data lake?, kafka?)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}