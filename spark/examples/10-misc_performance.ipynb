{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyDay-zKVX3g"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucprosa/dataeng-basic-course/blob/main/spark/examples/10-misc_performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOA_wQSmLd9z"
      },
      "source": [
        "# Miscellaneos Performance tricks\n",
        "- cache() & persist()\n",
        "- broadcast join\n",
        "- repartition & coalesce\n",
        "- explain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "c410e46c-4a50-43aa-926f-d0417c6280d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "637HFw00T3LP"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master('local').appName('Spark Course').getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj3Cg2riVX3m"
      },
      "source": [
        "# Preparing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "z-caHS2MVX3m"
      },
      "outputs": [],
      "source": [
        "from pyspark import SparkFiles\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Setting up URLs\n",
        "squirrel_url = \"https://raw.githubusercontent.com/lucprosa/dataeng-basic-course/main/data/squirrel-data/squirrel-data.csv\"\n",
        "park_url = \"https://raw.githubusercontent.com/lucprosa/dataeng-basic-course/main/data/squirrel-data/park-data.csv\"\n",
        "\n",
        "\n",
        "# Defining schemas\n",
        "squirrel_schema = StructType([\n",
        "StructField('Area Name',StringType(),True),\n",
        "StructField('Area ID',StringType(),True),\n",
        "StructField('Park Name',StringType(),True),\n",
        "StructField('Park ID', StringType(), True),\n",
        "StructField('Squirrel ID', StringType(), True),\n",
        "StructField('Primary Fur Color', StringType(), True),\n",
        "StructField('Highlights in Fur Color', StringType(), True),\n",
        "StructField('Color Notes', StringType(), True),\n",
        "StructField('Location', StringType(), True),\n",
        "StructField('Above Ground (Height in Feet)', StringType(), True),\n",
        "StructField('Specific Location', StringType(), True),\n",
        "StructField('Activities', StringType(), True),\n",
        "StructField('Interactions with Humans', StringType(), True),\n",
        "StructField('Squirrel Latitude (DD.DDDDDD)', StringType(), True),\n",
        "StructField('Squirrel Longitude (-DD.DDDDDD)', StringType(), True)\n",
        "])\n",
        "\n",
        "park_schema = StructType([\n",
        "StructField('Area Name',StringType(),True),\n",
        "StructField('Area ID',StringType(),True),\n",
        "StructField('Park Name',StringType(),True),\n",
        "StructField('Park ID', StringType(), True),\n",
        "StructField('Date', StringType(), True),\n",
        "StructField('Start Time', StringType(), True),\n",
        "StructField('End Time', StringType(), True),\n",
        "StructField('Total Time (in minutes, if available)', StringType(), True),\n",
        "StructField('Park Conditions', StringType(), True),\n",
        "StructField('Other Animal Sightings', StringType(), True),\n",
        "StructField('Litter', StringType(), True),\n",
        "StructField('Activities', StringType(), True),\n",
        "StructField('Temperature & Weather', StringType(), True),\n",
        "StructField('Number of Squirrels', IntegerType(), True),\n",
        "StructField('Squirrel Sighter(s)', StringType(), True),\n",
        "StructField('Number of Sighters', IntegerType(), True)\n",
        "])\n",
        "\n",
        "area_schema = StructType([\n",
        "StructField('Area ID',StringType(),True),\n",
        "StructField('Area Name',StringType(),True),\n",
        "StructField('Area Description',StringType(),True),\n",
        "StructField('City Name',StringType(),True),\n",
        "])\n",
        "\n",
        "area_data = [\n",
        "    (\"A\", \"UPPER MANHATTAN\", \"Uptown Manhattan\", \"New York\"),\n",
        "    (\"B\", \"CENTRAL MANHATTAN\", \"Midtown Manhattan\", \"New York\"),\n",
        "    (\"C\", \"LOWER MANHATTAN\", \"Downtown Manhattan\", \"New York\"),\n",
        "    (\"D\", \"BROOKLYN\", \"Brooklyn\", \"New York\")\n",
        "    ]\n",
        "\n",
        "spark.sparkContext.addFile(squirrel_url)\n",
        "spark.sparkContext.addFile(park_url)\n",
        "\n",
        "# creating dataframes\n",
        "squirrel = spark.read.csv(SparkFiles.get(\"squirrel-data.csv\"), header=True, schema=squirrel_schema)\n",
        "park = spark.read.csv(SparkFiles.get(\"park-data.csv\"), header=True, schema=park_schema)\n",
        "area = spark.createDataFrame(data=area_data, schema=area_schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "e1jkoWpweDqW",
        "outputId": "b76ccca2-bf37-4e38-f315-7943194a0c8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-------+-------------------+-------+-----------+-----------------+-----------------------+-----------+------------+-----------------------------+-----------------+--------------------+------------------------+-----------------------------+-------------------------------+\n",
            "|      Area Name|Area ID|          Park Name|Park ID|Squirrel ID|Primary Fur Color|Highlights in Fur Color|Color Notes|    Location|Above Ground (Height in Feet)|Specific Location|          Activities|Interactions with Humans|Squirrel Latitude (DD.DDDDDD)|Squirrel Longitude (-DD.DDDDDD)|\n",
            "+---------------+-------+-------------------+-------+-----------+-----------------+-----------------------+-----------+------------+-----------------------------+-----------------+--------------------+------------------------+-----------------------------+-------------------------------+\n",
            "|UPPER MANHATTAN|      A|    Fort Tryon Park|     01|    A-01-01|             Gray|                  White|       NULL|Ground Plane|                         NULL|             NULL|            Foraging|             Indifferent|                         NULL|                       40.85941|\n",
            "|UPPER MANHATTAN|      A|    Fort Tryon Park|     01|    A-01-02|             Gray|                  White|       NULL|Ground Plane|                         NULL|             NULL|            Foraging|             Indifferent|                 Looks skinny|                      40.859436|\n",
            "|UPPER MANHATTAN|      A|    Fort Tryon Park|     01|    A-01-03|             Gray|                  White|       NULL|Ground Plane|                         NULL|             NULL|Eating, Digging s...|             Indifferent|                         NULL|                      40.859416|\n",
            "|UPPER MANHATTAN|      A|    Fort Tryon Park|     01|    A-01-04|             Gray|                  White|       NULL|Ground Plane|                         NULL|             NULL|             Running|             Indifferent|                         NULL|                      40.859418|\n",
            "|UPPER MANHATTAN|      A|    Fort Tryon Park|     01|    A-01-05|             Gray|               Cinnamon|       NULL|Ground Plane|                         NULL|             NULL|     Running, Eating|             Indifferent|                She left food|                      40.859493|\n",
            "|UPPER MANHATTAN|      A|    Fort Tryon Park|     01|    A-01-06|             Gray|               Cinnamon|       NULL|Ground Plane|                         NULL|             NULL|            Climbing|             Indifferent|                         NULL|                      40.860825|\n",
            "|UPPER MANHATTAN|      A|    Fort Tryon Park|     01|    A-01-07|             Gray|                  White|       NULL|Ground Plane|                         NULL|             NULL|            Foraging|             Indifferent|                         NULL|                      40.860225|\n",
            "|UPPER MANHATTAN|      A|    Fort Tryon Park|     01|    A-01-08|            Black|                   Gray|       NULL|Above Ground|                           10|             NULL|            Climbing|               Runs From|                         NULL|                      40.859965|\n",
            "|UPPER MANHATTAN|      A|    Fort Tryon Park|     01|    A-01-09|             Gray|                  White|       NULL|Ground Plane|                         NULL|             NULL|            Foraging|             Indifferent|                         NULL|                      40.859892|\n",
            "|UPPER MANHATTAN|      A|    Fort Tryon Park|     01|    A-01-10|             Gray|                  White|       NULL|Ground Plane|                         NULL|             NULL|     Eating, Digging|             Indifferent|                         NULL|                      40.859636|\n",
            "|UPPER MANHATTAN|      A|    Fort Tryon Park|     01|    A-01-11|             Gray|                  Black|       NULL|Ground Plane|                         NULL|             NULL|     Eating, Digging|             Indifferent|         was intimidated b...|                      40.859576|\n",
            "|UPPER MANHATTAN|      A|    Fort Tryon Park|     01|    A-01-12|             Gray|                  White|       NULL|Ground Plane|                         NULL|             NULL|             Running|               Runs From|                         NULL|                      40.859989|\n",
            "|UPPER MANHATTAN|      A|J. Hood Wright Park|     02|    A-02-01|             Gray|                   Gray|       NULL|Ground Plane|                         NULL|             NULL|             Running|             Indifferent|                         NULL|                      40.845749|\n",
            "|UPPER MANHATTAN|      A|J. Hood Wright Park|     02|    A-02-02|             Gray|               Cinnamon|       NULL|Above Ground|                            2|             NULL|            Foraging|             Indifferent|                         NULL|                      40.845875|\n",
            "|UPPER MANHATTAN|      A|J. Hood Wright Park|     02|    A-02-03|             Gray|               Cinnamon|       NULL|Ground Plane|                         NULL|             NULL|            Foraging|                    NULL|                         NULL|                      40.845875|\n",
            "|UPPER MANHATTAN|      A|J. Hood Wright Park|     02|    A-02-04|             Gray|               Cinnamon|       NULL|Ground Plane|                         NULL|             NULL|             Running|             Indifferent|                         NULL|                      40.846088|\n",
            "|UPPER MANHATTAN|      A|J. Hood Wright Park|     02|    A-02-05|             Gray|               Cinnamon|       NULL|Ground Plane|                         NULL|             NULL|             Running|               Runs From|                         NULL|                      40.846088|\n",
            "|UPPER MANHATTAN|      A|J. Hood Wright Park|     02|    A-02-06|             Gray|               Cinnamon|       NULL|Ground Plane|                         NULL|             NULL|            Foraging|             Indifferent|                         NULL|                      40.846088|\n",
            "|UPPER MANHATTAN|      A|J. Hood Wright Park|     02|    A-02-07|             Gray|                   Gray|       NULL|Ground Plane|                         NULL|             NULL|                NULL|               Runs From|                         NULL|                      40.846222|\n",
            "|UPPER MANHATTAN|      A|J. Hood Wright Park|     02|    A-02-08|             Gray|               Cinnamon|       NULL|Ground Plane|                         NULL|             NULL|Foraging, Nesting...|             Indifferent|                         NULL|                      40.846222|\n",
            "+---------------+-------+-------------------+-------+-----------+-----------------+-----------------------+-----------+------------+-----------------------------+-----------------+--------------------+------------------------+-----------------------------+-------------------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-----------------+-------+--------------------+-------+------+----------+----------+-------------------------------------+--------------------+----------------------+--------------------+--------------------+---------------------+-------------------+-------------------+------------------+\n",
            "|        Area Name|Area ID|           Park Name|Park ID|  Date|Start Time|  End Time|Total Time (in minutes, if available)|     Park Conditions|Other Animal Sightings|              Litter|          Activities|Temperature & Weather|Number of Squirrels|Squirrel Sighter(s)|Number of Sighters|\n",
            "+-----------------+-------+--------------------+-------+------+----------+----------+-------------------------------------+--------------------+----------------------+--------------------+--------------------+---------------------+-------------------+-------------------+------------------+\n",
            "|  UPPER MANHATTAN|      A|     Fort Tryon Park|     01|3/1/20|3:14:00 PM|4:05:00 PM|                                   51|                Busy|  Humans, Dogs, Pig...|                Some|   43 degrees, sunny|                   12|               NULL|                  4|              NULL|\n",
            "|  UPPER MANHATTAN|      A| J. Hood Wright Park|     02|3/1/20|3:30:00 PM|4:00:00 PM|                                   30|                Calm|  Humans, Hawks, Do...|      Some, in trees|         cold, clear|                   24|               NULL|                  2|              NULL|\n",
            "|  UPPER MANHATTAN|      A|     Highbridge Park|     03|3/1/20|3:21:00 PM|4:15:00 PM|                                   54|Calm, pick-up bas...|  Humans, Dogs (3, ...|Some, especially ...|          43 degrees|                   16|               NULL|                  3|              NULL|\n",
            "|  UPPER MANHATTAN|      A|   St. Nicholas Park|     04|3/1/20|3:15:00 PM|3:45:00 PM|                                   30|                Calm|          Humans, Dogs|Some, backside of...|   43 degrees, clear|                   15|               NULL|                  3|              NULL|\n",
            "|  UPPER MANHATTAN|      A|Riverside Park (s...|     05|3/1/20|3:15:00 PM|3:45:00 PM|                                   30|                Calm|          Humans, Dogs|                NULL|                NULL|                   28|               NULL|                  3|              NULL|\n",
            "|  UPPER MANHATTAN|      A|  Marcus Garvey Park|     06|3/1/20|3:45:00 PM|4:15:00 PM|                                   30|Calm, re: humans,...|  Hawks, Dogs, Pigeons|            Abundant|   42 degrees, clear|                   34|                 16|                  1|              NULL|\n",
            "|CENTRAL MANHATTAN|      B| Madison Square Park|     07|3/1/20|2:30:00 PM|3:50:00 PM|                                   80|                Busy|  Humans, Dogs, Pig...|                NULL|   43 degrees, sunny|                   11|               NULL|                  4|              NULL|\n",
            "|CENTRAL MANHATTAN|      B|   Union Square Park|     08|3/1/20|3:15:00 PM|3:45:00 PM|                                   30|                Busy|  Humans, Dogs, Pig...|                NULL|   40 degrees, sunny|                   16|               NULL|                  4|              NULL|\n",
            "|CENTRAL MANHATTAN|      B|Stuyvesant Square...|     09|3/1/20|3:00:00 PM|4:00:00 PM|                                   60|Calm, 20���30 ppl...|  Humans, Dogs, Spa...|                Some|   45 degrees, sunny|                   25|               NULL|                  2|              NULL|\n",
            "|CENTRAL MANHATTAN|      B|Washington Square...|     10|3/1/20|3:20:00 PM|4:00:00 PM|                                   40|                Busy|          Humans, Dogs|                None|45 degrees, sunny...|                   51|               NULL|                  2|              NULL|\n",
            "|CENTRAL MANHATTAN|      B|Tompkins Square Park|     11|3/1/20|3:15:00 PM|3:45:00 PM|                                   30|                NULL|                  NULL|                NULL|                NULL|                   59|               NULL|                  2|              NULL|\n",
            "|CENTRAL MANHATTAN|      B|John V. Lindsay E...|     12|3/1/20|3:01:00 PM|3:45:00 PM|                                   44|                Calm|  Humans (Joggers, ...|                NULL|        windy, clear|                   12|                 31|                  1|              NULL|\n",
            "|  LOWER MANHATTAN|      C|Sara D. Roosevelt...|   13.1|3/1/20|3:30:00 PM|4:00:00 PM|                                   30|                Busy|   Humans, Dogs (Gray)|                Some|   44 degrees, sunny|                    0|               NULL|                  3|              NULL|\n",
            "|  LOWER MANHATTAN|      C|Sara D. Roosevelt...|   13.2|3/1/20|3:30:00 PM|4:00:00 PM|                                   30|                Busy|       Humans, Pigeons|                Some|   43 degrees, sunny|                    0|               NULL|                  3|              NULL|\n",
            "|  LOWER MANHATTAN|      C|         Seward Park|     14|3/1/20|3:25:00 PM|3:55:00 PM|                                   30|                Busy|       Humans, Pigeons|                Some|   40 degrees, sunny|                    7|               NULL|                  4|              NULL|\n",
            "|  LOWER MANHATTAN|      C|  Corlears Hook Park|     15|3/1/20|3:35:00 PM|4:15:00 PM|                                   40|                Calm|  Humans, Dogs, Pig...|Some, mostly in t...|   48 degrees, sunny|                   16|               NULL|                  3|              NULL|\n",
            "|  LOWER MANHATTAN|      C|       Columbus Park|     16|3/1/20|3:47:00 PM|4:38:00 PM|                                   51|                Busy|  Humans, Dogs, Pig...|                None|42 degrees, windy...|                    4|               NULL|                  2|              NULL|\n",
            "|  LOWER MANHATTAN|      C|   Thomas Paine Park|     17|3/1/20|3:35:00 PM|3:45:00 PM|                                   10|                Calm|  Humans, Dogs, Pig...|                None|42 degrees, windy...|                    0|               NULL|                  2|              NULL|\n",
            "|  LOWER MANHATTAN|      C|       Teardrop Park|     18|3/1/20|3:37:00 PM|4:00:00 PM|                                   23|                NULL|  Humans, Dogs, Fis...|                Some|43 degrees, sunny...|                    1|               NULL|                  2|              NULL|\n",
            "|  LOWER MANHATTAN|      C|      City Hall Park|     19|3/1/20|3:34:00 PM|4:04:00 PM|                                   30|                Calm|  Humans, Pigeons, Cat|                None|   44 degrees, sunny|                   18|               NULL|                  2|              NULL|\n",
            "+-----------------+-------+--------------------+-------+------+----------+----------+-------------------------------------+--------------------+----------------------+--------------------+--------------------+---------------------+-------------------+-------------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "+-------+-----------------+------------------+---------+\n",
            "|Area ID|        Area Name|  Area Description|City Name|\n",
            "+-------+-----------------+------------------+---------+\n",
            "|      A|  UPPER MANHATTAN|  Uptown Manhattan| New York|\n",
            "|      B|CENTRAL MANHATTAN| Midtown Manhattan| New York|\n",
            "|      C|  LOWER MANHATTAN|Downtown Manhattan| New York|\n",
            "|      D|         BROOKLYN|          Brooklyn| New York|\n",
            "+-------+-----------------+------------------+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# show data\n",
        "squirrel.show()\n",
        "park.show()\n",
        "area.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Caching & Persist"
      ],
      "metadata": {
        "id": "9xipEQu1aUcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Caching\n",
        "# Default: MEMORY_AND_DISK\n",
        "\n",
        "import uuid\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "@udf\n",
        "def generate_uuid():\n",
        "  return str(uuid.uuid4())\n",
        "\n",
        "# transformation 1\n",
        "squirrel = squirrel.withColumn(\"hash_id\", generate_uuid())\n",
        "\n",
        "# transformation 2\n",
        "squirrel = squirrel.dropDuplicates()\n",
        "\n",
        "squirrel.cache().count() # <--------------- force an action to run the cache\n",
        "\n",
        "# transformations N\n",
        "# squirrel = squirrel.join...\n",
        "# squirrel = squirrel.groupBy...\n",
        "\n",
        "# DAG\n",
        "# T1 -> T2 -> T3...TN -> A1\n",
        "\n",
        "# action 1\n",
        "# squirrel.write.format(\"parquet\").path(\"path\")\n",
        "\n"
      ],
      "metadata": {
        "id": "mOOh51r6WmDj",
        "outputId": "7a0873f5-55b5-4562-a29f-6bcef665a76b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "433"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "squirrel.is_cached"
      ],
      "metadata": {
        "id": "NTrdLhFlY0NU",
        "outputId": "585438e8-74e9-4491-f4cc-900fd86bf233",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yEpZwS_QtJ1",
        "outputId": "c33621db-f62a-4058-b29c-471dc21e53e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------+-------+--------------------+-------+-----------+-----------------+-----------------------+-----------+--------------------+-----------------------------+-----------------+--------------------+------------------------+-----------------------------+-------------------------------+--------------------+\n",
            "|        Area Name|Area ID|           Park Name|Park ID|Squirrel ID|Primary Fur Color|Highlights in Fur Color|Color Notes|            Location|Above Ground (Height in Feet)|Specific Location|          Activities|Interactions with Humans|Squirrel Latitude (DD.DDDDDD)|Squirrel Longitude (-DD.DDDDDD)|             hash_id|\n",
            "+-----------------+-------+--------------------+-------+-----------+-----------------+-----------------------+-----------+--------------------+-----------------------------+-----------------+--------------------+------------------------+-----------------------------+-------------------------------+--------------------+\n",
            "|CENTRAL MANHATTAN|      B|Stuyvesant Square...|     09|    B-09-12|             Gray|               Cinnamon|       NULL|        Above Ground|                           20|             NULL|             Chasing|             Indifferent|         Juvenile? Playing...|                      40.733648|e3ee39e9-2894-409...|\n",
            "|  LOWER MANHATTAN|      C|  Corlears Hook Park|     15|    C-15-03|             Gray|                   NULL|       NULL|                NULL|                         NULL|             NULL|                NULL|                    NULL|         Multiple nut dive...|                      40.712118|e0ad6f1c-d19e-400...|\n",
            "|  UPPER MANHATTAN|      A|     Fort Tryon Park|     01|    A-01-10|             Gray|                  White|       NULL|        Ground Plane|                         NULL|             NULL|     Eating, Digging|             Indifferent|                         NULL|                      40.859636|6e16af76-85fe-471...|\n",
            "|CENTRAL MANHATTAN|      B| Madison Square Park|     07|    B-07-04|             Gray|                   NULL|       NULL|Above Ground, Spe...|                           10|          in tree|     Resting in tree|                    NULL|                         NULL|                      40.742792|b78d24df-17bc-4d5...|\n",
            "|  UPPER MANHATTAN|      A|     Fort Tryon Park|     01|    A-01-05|             Gray|               Cinnamon|       NULL|        Ground Plane|                         NULL|             NULL|     Running, Eating|             Indifferent|                She left food|                      40.859493|da3f14e0-9832-44c...|\n",
            "|  UPPER MANHATTAN|      A|Riverside Park (S...|     05|    A-05-20|             Gray|                   Gray|       NULL|        Ground Plane|                         NULL|             NULL|             Running|             Indifferent|                         NULL|                           NULL|5d5d125d-117f-41b...|\n",
            "|  UPPER MANHATTAN|      A|     Highbridge Park|     03|    A-03-11|             Gray|               Cinnamon|       NULL|        Above Ground|                            4|             NULL|  Vocalization at us|    Approaches, watch...|                         NULL|                      40.842673|7d2ab89c-703e-4fa...|\n",
            "|  UPPER MANHATTAN|      A|     Fort Tryon Park|     01|    A-01-03|             Gray|                  White|       NULL|        Ground Plane|                         NULL|             NULL|Eating, Digging s...|             Indifferent|                         NULL|                      40.859416|c250acf2-6c60-4b7...|\n",
            "|CENTRAL MANHATTAN|      B|John V. Lindsay E...|     12|    B-12-02|            Black|                  Black|       NULL|        Ground Plane|                         NULL|             NULL|Running, Eating (...|                    NULL|                         NULL|                      40.722614|5d1a3550-2cdf-40c...|\n",
            "|         BROOKLYN|      D|       McCarren Park|     22|    D-22-27|             Gray|               Cinnamon|       NULL|        Ground Plane|                         NULL|             NULL|Eating (bread cru...|             Indifferent|         Near a lot of lit...|                      40.719777|081952cd-0d2a-415...|\n",
            "|  UPPER MANHATTAN|      A|Riverside Park (S...|     05|    A-05-04|             Gray|                   Gray|       NULL|        Ground Plane|                         NULL|             NULL|             Chasing|             Indifferent|                         NULL|                           NULL|c89d449f-b131-47c...|\n",
            "|CENTRAL MANHATTAN|      B|Tompkins Square Park|     11|    B-11-51|            Black|                  Black|       NULL|        Ground Plane|                         NULL|             NULL|             Chasing|             Indifferent|                         NULL|                           NULL|2f570a1a-adf7-4bf...|\n",
            "|CENTRAL MANHATTAN|      B|Stuyvesant Square...|     09|    B-09-04|             Gray|               Cinnamon|       NULL|        Ground Plane|                         NULL|             NULL|            Climbing|               Runs From|                         NULL|                      40.733546|d6a6f44f-37b9-448...|\n",
            "|  UPPER MANHATTAN|      A|Riverside Park (S...|     05|    A-05-22|             Gray|                   Gray|       NULL|        Ground Plane|                         NULL|             NULL|             Chasing|                    NULL|                         NULL|                           NULL|02089ea9-6200-42c...|\n",
            "|         BROOKLYN|      D|       McCarren Park|     22|    D-22-31|         Cinnamon|                  White|       NULL|        Ground Plane|                         NULL|             NULL|            Foraging|             Indifferent|         Walking around bu...|                      40.720412|3a91ef52-726f-4b7...|\n",
            "|CENTRAL MANHATTAN|      B|Tompkins Square Park|     11|    B-11-50|             Gray|                   Gray|       NULL|        Ground Plane|                         NULL|             NULL|             Chasing|             Indifferent|                         NULL|                           NULL|3ff959a7-35c5-4b6...|\n",
            "|  LOWER MANHATTAN|      C|  Corlears Hook Park|     15|    C-15-06|             Gray|                   NULL|       NULL|        Above Ground|                           15|             NULL|      Ear scratching|                    NULL|                         NULL|                      40.712224|6e9e26e5-6592-47e...|\n",
            "|         BROOKLYN|      D|       McCarren Park|     22|    D-22-22|             Gray|        Cinnamon, White|       NULL|        Above Ground|                           20|             NULL|            Climbing|             Indifferent|         One of three toge...|                      40.720533|fc6b959f-82d7-466...|\n",
            "|         BROOKLYN|      D|Msgr. McGolrick Park|     21|    D-21-09|             Gray|                   NULL|       NULL|        Ground Plane|                         NULL|             NULL|     Climbing (down)|                    NULL|                         NULL|                      40.723974|8ebcf0b2-48c5-415...|\n",
            "|  LOWER MANHATTAN|      C|  Corlears Hook Park|     15|    C-15-07|             Gray|                   NULL|       NULL|        Above Ground|                           25|             NULL|                NULL|                    NULL|                         NULL|                       40.71224|9d7adf13-d93f-4d4...|\n",
            "+-----------------+-------+--------------------+-------+-----------+-----------------+-----------------------+-----------+--------------------+-----------------------------+-----------------+--------------------+------------------------+-----------------------------+-------------------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "squirrel.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "squirrel.unpersist()"
      ],
      "metadata": {
        "id": "Cyol21etZ4EO",
        "outputId": "d76903d0-a62c-4426-92c2-f0fa5b7e0384",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Area Name: string, Area ID: string, Park Name: string, Park ID: string, Squirrel ID: string, Primary Fur Color: string, Highlights in Fur Color: string, Color Notes: string, Location: string, Above Ground (Height in Feet): string, Specific Location: string, Activities: string, Interactions with Humans: string, Squirrel Latitude (DD.DDDDDD): string, Squirrel Longitude (-DD.DDDDDD): string, hash_id: string]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Persist\n",
        "# Default: MEMORY_ONLY\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import StorageLevel\n",
        "\n",
        "# first execution plan\n",
        "print(area.explain(\"cost\"))\n",
        "\n",
        "area = area.withColumn(\"City shortname\", lit(\"NY\"))\n",
        "# second execution plan\n",
        "print(area.explain(\"cost\"))\n",
        "\n",
        "area = area.persist(StorageLevel.MEMORY_ONLY)\n",
        "area.count()\n",
        "\n",
        "# second execution plan\n",
        "area2 = area.withColumn(\"Teste\", lit(\"test\"))\n",
        "print(area2.explain(\"cost\"))\n",
        "\n",
        "print(area.storageLevel)\n",
        "print(area.is_cached)"
      ],
      "metadata": {
        "id": "cz63P4OGaR3T",
        "outputId": "72907518-c6c2-4c12-86cb-fee357f1cf8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Optimized Logical Plan ==\n",
            "LogicalRDD [Area ID#62, Area Name#63, Area Description#64, City Name#65], false, Statistics(sizeInBytes=8.0 EiB)\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Scan ExistingRDD[Area ID#62,Area Name#63,Area Description#64,City Name#65]\n",
            "\n",
            "\n",
            "None\n",
            "== Optimized Logical Plan ==\n",
            "Project [Area ID#62, Area Name#63, Area Description#64, City Name#65, NY AS City shortname#2086], Statistics(sizeInBytes=9.8 EiB)\n",
            "+- LogicalRDD [Area ID#62, Area Name#63, Area Description#64, City Name#65], false, Statistics(sizeInBytes=8.0 EiB)\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [Area ID#62, Area Name#63, Area Description#64, City Name#65, NY AS City shortname#2086]\n",
            "+- *(1) Scan ExistingRDD[Area ID#62,Area Name#63,Area Description#64,City Name#65]\n",
            "\n",
            "\n",
            "None\n",
            "== Optimized Logical Plan ==\n",
            "Project [Area ID#62, Area Name#63, Area Description#64, City Name#65, City shortname#2086, test AS Teste#2252], Statistics(sizeInBytes=282.0 B)\n",
            "+- InMemoryRelation [Area ID#62, Area Name#63, Area Description#64, City Name#65, City shortname#2086], StorageLevel(memory, 1 replicas), Statistics(sizeInBytes=238.0 B, rowCount=4)\n",
            "      +- *(1) Project [Area ID#62, Area Name#63, Area Description#64, City Name#65, NY AS City shortname#2086]\n",
            "         +- *(1) Scan ExistingRDD[Area ID#62,Area Name#63,Area Description#64,City Name#65]\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [Area ID#62, Area Name#63, Area Description#64, City Name#65, City shortname#2086, test AS Teste#2252]\n",
            "   +- InMemoryTableScan [Area Description#64, Area ID#62, Area Name#63, City Name#65, City shortname#2086]\n",
            "         +- InMemoryRelation [Area ID#62, Area Name#63, Area Description#64, City Name#65, City shortname#2086], StorageLevel(memory, 1 replicas)\n",
            "               +- *(1) Project [Area ID#62, Area Name#63, Area Description#64, City Name#65, NY AS City shortname#2086]\n",
            "                  +- *(1) Scan ExistingRDD[Area ID#62,Area Name#63,Area Description#64,City Name#65]\n",
            "\n",
            "\n",
            "None\n",
            "Memory Serialized 1x Replicated\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Persist\n",
        "# Default: MEMORY_AND_DISK\n",
        "\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import StorageLevel\n",
        "\n",
        "# first execution plan\n",
        "print(area.explain(\"cost\"))\n",
        "\n",
        "area = area.withColumn(\"City shortname\", lit(\"NY\"))\n",
        "# second execution plan\n",
        "print(area.explain(\"cost\"))\n",
        "\n",
        "area = area.persist(StorageLevel.DISK_ONLY)\n",
        "area.count()\n",
        "\n",
        "# second execution plan\n",
        "area2 = area.withColumn(\"Teste\", lit(\"test\"))\n",
        "print(area2.explain(\"cost\"))\n",
        "\n",
        "print(area.storageLevel)\n",
        "print(area.is_cached)"
      ],
      "metadata": {
        "id": "Y8lWwqd4bK5Z",
        "outputId": "be3b9a5b-04e2-4057-8ee7-b627c662ffb6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Optimized Logical Plan ==\n",
            "Project [Area ID#62, Area Name#63, Area Description#64, City Name#65, NY AS City shortname#2086], Statistics(sizeInBytes=9.8 EiB)\n",
            "+- LogicalRDD [Area ID#62, Area Name#63, Area Description#64, City Name#65], false, Statistics(sizeInBytes=8.0 EiB)\n",
            "\n",
            "== Physical Plan ==\n",
            "*(1) Project [Area ID#62, Area Name#63, Area Description#64, City Name#65, NY AS City shortname#2086]\n",
            "+- *(1) Scan ExistingRDD[Area ID#62,Area Name#63,Area Description#64,City Name#65]\n",
            "\n",
            "\n",
            "None\n",
            "== Optimized Logical Plan ==\n",
            "Project [Area ID#62, Area Name#63, Area Description#64, City Name#65, NY AS City shortname#2334], Statistics(sizeInBytes=238.0 B)\n",
            "+- InMemoryRelation [Area ID#62, Area Name#63, Area Description#64, City Name#65, City shortname#2086], StorageLevel(memory, 1 replicas), Statistics(sizeInBytes=238.0 B, rowCount=4)\n",
            "      +- *(1) Project [Area ID#62, Area Name#63, Area Description#64, City Name#65, NY AS City shortname#2086]\n",
            "         +- *(1) Scan ExistingRDD[Area ID#62,Area Name#63,Area Description#64,City Name#65]\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [Area ID#62, Area Name#63, Area Description#64, City Name#65, NY AS City shortname#2334]\n",
            "   +- InMemoryTableScan [Area Description#64, Area ID#62, Area Name#63, City Name#65]\n",
            "         +- InMemoryRelation [Area ID#62, Area Name#63, Area Description#64, City Name#65, City shortname#2086], StorageLevel(memory, 1 replicas)\n",
            "               +- *(1) Project [Area ID#62, Area Name#63, Area Description#64, City Name#65, NY AS City shortname#2086]\n",
            "                  +- *(1) Scan ExistingRDD[Area ID#62,Area Name#63,Area Description#64,City Name#65]\n",
            "\n",
            "\n",
            "None\n",
            "== Optimized Logical Plan ==\n",
            "Project [Area ID#62, Area Name#63, Area Description#64, City Name#65, City shortname#2334, test AS Teste#2675], Statistics(sizeInBytes=282.0 B)\n",
            "+- InMemoryRelation [Area ID#62, Area Name#63, Area Description#64, City Name#65, City shortname#2334], StorageLevel(disk, 1 replicas), Statistics(sizeInBytes=238.0 B, rowCount=4)\n",
            "      +- AdaptiveSparkPlan isFinalPlan=true\n",
            "         +- == Final Plan ==\n",
            "            *(1) Project [Area ID#62, Area Name#63, Area Description#64, City Name#65, NY AS City shortname#2334]\n",
            "            +- TableCacheQueryStage 0\n",
            "               +- InMemoryTableScan [Area Description#64, Area ID#62, Area Name#63, City Name#65]\n",
            "                     +- InMemoryRelation [Area ID#62, Area Name#63, Area Description#64, City Name#65, City shortname#2086], StorageLevel(memory, 1 replicas)\n",
            "                           +- *(1) Project [Area ID#62, Area Name#63, Area Description#64, City Name#65, NY AS City shortname#2086]\n",
            "                              +- *(1) Scan ExistingRDD[Area ID#62,Area Name#63,Area Description#64,City Name#65]\n",
            "         +- == Initial Plan ==\n",
            "            Project [Area ID#62, Area Name#63, Area Description#64, City Name#65, NY AS City shortname#2334]\n",
            "            +- InMemoryTableScan [Area Description#64, Area ID#62, Area Name#63, City Name#65]\n",
            "                  +- InMemoryRelation [Area ID#62, Area Name#63, Area Description#64, City Name#65, City shortname#2086], StorageLevel(memory, 1 replicas)\n",
            "                        +- *(1) Project [Area ID#62, Area Name#63, Area Description#64, City Name#65, NY AS City shortname#2086]\n",
            "                           +- *(1) Scan ExistingRDD[Area ID#62,Area Name#63,Area Description#64,City Name#65]\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [Area ID#62, Area Name#63, Area Description#64, City Name#65, City shortname#2334, test AS Teste#2675]\n",
            "   +- InMemoryTableScan [Area Description#64, Area ID#62, Area Name#63, City Name#65, City shortname#2334]\n",
            "         +- InMemoryRelation [Area ID#62, Area Name#63, Area Description#64, City Name#65, City shortname#2334], StorageLevel(disk, 1 replicas)\n",
            "               +- AdaptiveSparkPlan isFinalPlan=true\n",
            "                  +- == Final Plan ==\n",
            "                     *(1) Project [Area ID#62, Area Name#63, Area Description#64, City Name#65, NY AS City shortname#2334]\n",
            "                     +- TableCacheQueryStage 0\n",
            "                        +- InMemoryTableScan [Area Description#64, Area ID#62, Area Name#63, City Name#65]\n",
            "                              +- InMemoryRelation [Area ID#62, Area Name#63, Area Description#64, City Name#65, City shortname#2086], StorageLevel(memory, 1 replicas)\n",
            "                                    +- *(1) Project [Area ID#62, Area Name#63, Area Description#64, City Name#65, NY AS City shortname#2086]\n",
            "                                       +- *(1) Scan ExistingRDD[Area ID#62,Area Name#63,Area Description#64,City Name#65]\n",
            "                  +- == Initial Plan ==\n",
            "                     Project [Area ID#62, Area Name#63, Area Description#64, City Name#65, NY AS City shortname#2334]\n",
            "                     +- InMemoryTableScan [Area Description#64, Area ID#62, Area Name#63, City Name#65]\n",
            "                           +- InMemoryRelation [Area ID#62, Area Name#63, Area Description#64, City Name#65, City shortname#2086], StorageLevel(memory, 1 replicas)\n",
            "                                 +- *(1) Project [Area ID#62, Area Name#63, Area Description#64, City Name#65, NY AS City shortname#2086]\n",
            "                                    +- *(1) Scan ExistingRDD[Area ID#62,Area Name#63,Area Description#64,City Name#65]\n",
            "\n",
            "\n",
            "None\n",
            "Disk Serialized 1x Replicated\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Broadcast Join"
      ],
      "metadata": {
        "id": "jG6-tBXIocwm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Tshz4hV3S2Tx",
        "outputId": "6b22fde9-c577-4422-f198-eada1638428d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Project [Area Description#64, Park Name#32, Date#34, Squirrel ID#4]\n",
            "   +- BroadcastHashJoin [Area ID#1], [Area ID#62], Inner, BuildRight, false\n",
            "      :- Project [Area ID#1, Squirrel ID#4, Park Name#32, Date#34]\n",
            "      :  +- BroadcastHashJoin [Park ID#3], [Park ID#33], Inner, BuildRight, false\n",
            "      :     :- HashAggregate(keys=[Above Ground (Height in Feet)#9, Primary Fur Color#5, hash_id#782, Location#8, Park ID#3, Specific Location#10, Squirrel ID#4, Area ID#1, Activities#11, Squirrel Latitude (DD.DDDDDD)#13, Color Notes#7, Area Name#0, Highlights in Fur Color#6, Interactions with Humans#12, Squirrel Longitude (-DD.DDDDDD)#14, Park Name#2], functions=[])\n",
            "      :     :  +- Exchange hashpartitioning(Above Ground (Height in Feet)#9, Primary Fur Color#5, hash_id#782, Location#8, Park ID#3, Specific Location#10, Squirrel ID#4, Area ID#1, Activities#11, Squirrel Latitude (DD.DDDDDD)#13, Color Notes#7, Area Name#0, Highlights in Fur Color#6, Interactions with Humans#12, Squirrel Longitude (-DD.DDDDDD)#14, Park Name#2, 200), ENSURE_REQUIREMENTS, [plan_id=820]\n",
            "      :     :     +- HashAggregate(keys=[Above Ground (Height in Feet)#9, Primary Fur Color#5, hash_id#782, Location#8, Park ID#3, Specific Location#10, Squirrel ID#4, Area ID#1, Activities#11, Squirrel Latitude (DD.DDDDDD)#13, Color Notes#7, Area Name#0, Highlights in Fur Color#6, Interactions with Humans#12, Squirrel Longitude (-DD.DDDDDD)#14, Park Name#2], functions=[])\n",
            "      :     :        +- Project [Area Name#0, Area ID#1, Park Name#2, Park ID#3, Squirrel ID#4, Primary Fur Color#5, Highlights in Fur Color#6, Color Notes#7, Location#8, Above Ground (Height in Feet)#9, Specific Location#10, Activities#11, Interactions with Humans#12, Squirrel Latitude (DD.DDDDDD)#13, Squirrel Longitude (-DD.DDDDDD)#14, pythonUDF0#3994 AS hash_id#782]\n",
            "      :     :           +- BatchEvalPython [generate_uuid()#781], [pythonUDF0#3994]\n",
            "      :     :              +- Filter (isnotnull(Park ID#3) AND isnotnull(Area ID#1))\n",
            "      :     :                 +- InMemoryTableScan [Area Name#0, Area ID#1, Park Name#2, Park ID#3, Squirrel ID#4, Primary Fur Color#5, Highlights in Fur Color#6, Color Notes#7, Location#8, Above Ground (Height in Feet)#9, Specific Location#10, Activities#11, Interactions with Humans#12, Squirrel Latitude (DD.DDDDDD)#13, Squirrel Longitude (-DD.DDDDDD)#14], [isnotnull(Park ID#3), isnotnull(Area ID#1)]\n",
            "      :     :                       +- InMemoryRelation [Area Name#0, Area ID#1, Park Name#2, Park ID#3, Squirrel ID#4, Primary Fur Color#5, Highlights in Fur Color#6, Color Notes#7, Location#8, Above Ground (Height in Feet)#9, Specific Location#10, Activities#11, Interactions with Humans#12, Squirrel Latitude (DD.DDDDDD)#13, Squirrel Longitude (-DD.DDDDDD)#14, hash_id#247], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
            "      :     :                             +- AdaptiveSparkPlan isFinalPlan=true\n",
            "                                             +- == Final Plan ==\n",
            "                                                *(2) HashAggregate(keys=[Above Ground (Height in Feet)#9, Primary Fur Color#5, hash_id#247, Location#8, Park ID#3, Specific Location#10, Squirrel ID#4, Area ID#1, Activities#11, Squirrel Latitude (DD.DDDDDD)#13, Color Notes#7, Area Name#0, Highlights in Fur Color#6, Interactions with Humans#12, Squirrel Longitude (-DD.DDDDDD)#14, Park Name#2], functions=[])\n",
            "                                                +- ShuffleQueryStage 0\n",
            "                                                   +- Exchange hashpartitioning(Above Ground (Height in Feet)#9, Primary Fur Color#5, hash_id#247, Location#8, Park ID#3, Specific Location#10, Squirrel ID#4, Area ID#1, Activities#11, Squirrel Latitude (DD.DDDDDD)#13, Color Notes#7, Area Name#0, Highlights in Fur Color#6, Interactions with Humans#12, Squirrel Longitude (-DD.DDDDDD)#14, Park Name#2, 200), ENSURE_REQUIREMENTS, [plan_id=91]\n",
            "                                                      +- *(1) HashAggregate(keys=[Above Ground (Height in Feet)#9, Primary Fur Color#5, hash_id#247, Location#8, Park ID#3, Specific Location#10, Squirrel ID#4, Area ID#1, Activities#11, Squirrel Latitude (DD.DDDDDD)#13, Color Notes#7, Area Name#0, Highlights in Fur Color#6, Interactions with Humans#12, Squirrel Longitude (-DD.DDDDDD)#14, Park Name#2], functions=[])\n",
            "                                                         +- *(1) Project [Area Name#0, Area ID#1, Park Name#2, Park ID#3, Squirrel ID#4, Primary Fur Color#5, Highlights in Fur Color#6, Color Notes#7, Location#8, Above Ground (Height in Feet)#9, Specific Location#10, Activities#11, Interactions with Humans#12, Squirrel Latitude (DD.DDDDDD)#13, Squirrel Longitude (-DD.DDDDDD)#14, pythonUDF0#264 AS hash_id#247]\n",
            "                                                            +- BatchEvalPython [generate_uuid()#246], [pythonUDF0#264]\n",
            "                                                               +- FileScan csv [Area Name#0,Area ID#1,Park Name#2,Park ID#3,Squirrel ID#4,Primary Fur Color#5,Highlights in Fur Color#6,Color Notes#7,Location#8,Above Ground (Height in Feet)#9,Specific Location#10,Activities#11,Interactions with Humans#12,Squirrel Latitude (DD.DDDDDD)#13,Squirrel Longitude (-DD.DDDDDD)#14] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/tmp/spark-0091e21d-5ce8-4a1f-bb20-0a649bf6a3e0/userFiles-ea02405..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Area Name:string,Area ID:string,Park Name:string,Park ID:string,Squirrel ID:string,Primary...\n",
            "                                             +- == Initial Plan ==\n",
            "                                                HashAggregate(keys=[Above Ground (Height in Feet)#9, Primary Fur Color#5, hash_id#247, Location#8, Park ID#3, Specific Location#10, Squirrel ID#4, Area ID#1, Activities#11, Squirrel Latitude (DD.DDDDDD)#13, Color Notes#7, Area Name#0, Highlights in Fur Color#6, Interactions with Humans#12, Squirrel Longitude (-DD.DDDDDD)#14, Park Name#2], functions=[])\n",
            "                                                +- Exchange hashpartitioning(Above Ground (Height in Feet)#9, Primary Fur Color#5, hash_id#247, Location#8, Park ID#3, Specific Location#10, Squirrel ID#4, Area ID#1, Activities#11, Squirrel Latitude (DD.DDDDDD)#13, Color Notes#7, Area Name#0, Highlights in Fur Color#6, Interactions with Humans#12, Squirrel Longitude (-DD.DDDDDD)#14, Park Name#2, 200), ENSURE_REQUIREMENTS, [plan_id=55]\n",
            "                                                   +- HashAggregate(keys=[Above Ground (Height in Feet)#9, Primary Fur Color#5, hash_id#247, Location#8, Park ID#3, Specific Location#10, Squirrel ID#4, Area ID#1, Activities#11, Squirrel Latitude (DD.DDDDDD)#13, Color Notes#7, Area Name#0, Highlights in Fur Color#6, Interactions with Humans#12, Squirrel Longitude (-DD.DDDDDD)#14, Park Name#2], functions=[])\n",
            "                                                      +- Project [Area Name#0, Area ID#1, Park Name#2, Park ID#3, Squirrel ID#4, Primary Fur Color#5, Highlights in Fur Color#6, Color Notes#7, Location#8, Above Ground (Height in Feet)#9, Specific Location#10, Activities#11, Interactions with Humans#12, Squirrel Latitude (DD.DDDDDD)#13, Squirrel Longitude (-DD.DDDDDD)#14, pythonUDF0#264 AS hash_id#247]\n",
            "                                                         +- BatchEvalPython [generate_uuid()#246], [pythonUDF0#264]\n",
            "                                                            +- FileScan csv [Area Name#0,Area ID#1,Park Name#2,Park ID#3,Squirrel ID#4,Primary Fur Color#5,Highlights in Fur Color#6,Color Notes#7,Location#8,Above Ground (Height in Feet)#9,Specific Location#10,Activities#11,Interactions with Humans#12,Squirrel Latitude (DD.DDDDDD)#13,Squirrel Longitude (-DD.DDDDDD)#14] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/tmp/spark-0091e21d-5ce8-4a1f-bb20-0a649bf6a3e0/userFiles-ea02405..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Area Name:string,Area ID:string,Park Name:string,Park ID:string,Squirrel ID:string,Primary...\n",
            "      :     +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, false]),false), [plan_id=823]\n",
            "      :        +- Filter isnotnull(Park ID#33)\n",
            "      :           +- FileScan csv [Park Name#32,Park ID#33,Date#34] Batched: false, DataFilters: [isnotnull(Park ID#33)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/tmp/spark-0091e21d-5ce8-4a1f-bb20-0a649bf6a3e0/userFiles-ea02405..., PartitionFilters: [], PushedFilters: [IsNotNull(Park ID)], ReadSchema: struct<Park Name:string,Park ID:string,Date:string>\n",
            "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=827]\n",
            "         +- Filter isnotnull(Area ID#62)\n",
            "            +- InMemoryTableScan [Area ID#62, Area Description#64], [isnotnull(Area ID#62)]\n",
            "                  +- InMemoryRelation [Area ID#62, Area Name#63, Area Description#64, City Name#65, City shortname#2334], StorageLevel(disk, 1 replicas)\n",
            "                        +- AdaptiveSparkPlan isFinalPlan=true\n",
            "                           +- == Final Plan ==\n",
            "                              *(1) Project [Area ID#62, Area Name#63, Area Description#64, City Name#65, NY AS City shortname#2334]\n",
            "                              +- TableCacheQueryStage 0\n",
            "                                 +- InMemoryTableScan [Area Description#64, Area ID#62, Area Name#63, City Name#65]\n",
            "                                       +- InMemoryRelation [Area ID#62, Area Name#63, Area Description#64, City Name#65, City shortname#2086], StorageLevel(memory, 1 replicas)\n",
            "                                             +- *(1) Project [Area ID#62, Area Name#63, Area Description#64, City Name#65, NY AS City shortname#2086]\n",
            "                                                +- *(1) Scan ExistingRDD[Area ID#62,Area Name#63,Area Description#64,City Name#65]\n",
            "                           +- == Initial Plan ==\n",
            "                              Project [Area ID#62, Area Name#63, Area Description#64, City Name#65, NY AS City shortname#2334]\n",
            "                              +- InMemoryTableScan [Area Description#64, Area ID#62, Area Name#63, City Name#65]\n",
            "                                    +- InMemoryRelation [Area ID#62, Area Name#63, Area Description#64, City Name#65, City shortname#2086], StorageLevel(memory, 1 replicas)\n",
            "                                          +- *(1) Project [Area ID#62, Area Name#63, Area Description#64, City Name#65, NY AS City shortname#2086]\n",
            "                                             +- *(1) Scan ExistingRDD[Area ID#62,Area Name#63,Area Description#64,City Name#65]\n",
            "\n",
            "\n",
            "+------------------+--------------------+------+-----------+\n",
            "|  Area Description|           Park Name|  Date|Squirrel ID|\n",
            "+------------------+--------------------+------+-----------+\n",
            "|          Brooklyn|       McCarren Park|3/1/20|    D-22-16|\n",
            "|  Uptown Manhattan|   St. Nicholas Park|3/1/20|    A-04-09|\n",
            "| Midtown Manhattan|Tompkins Square Park|3/1/20|    B-11-36|\n",
            "| Midtown Manhattan|Washington Square...|3/1/20|    B-10-43|\n",
            "|          Brooklyn|Msgr. McGolrick Park|3/1/20|    D-21-09|\n",
            "|          Brooklyn|       McCarren Park|3/1/20|    D-22-24|\n",
            "|  Uptown Manhattan|Riverside Park (s...|3/1/20|    A-05-04|\n",
            "| Midtown Manhattan|Washington Square...|3/1/20|    B-10-34|\n",
            "|Downtown Manhattan|      City Hall Park|3/1/20|    C-19-02|\n",
            "| Midtown Manhattan|Washington Square...|3/1/20|    B-10-10|\n",
            "|Downtown Manhattan|         Seward Park|3/1/20|    C-14-06|\n",
            "|  Uptown Manhattan|Riverside Park (s...|3/1/20|    A-05-27|\n",
            "|  Uptown Manhattan|  Marcus Garvey Park|3/1/20|    A-06-23|\n",
            "|  Uptown Manhattan|Riverside Park (s...|3/1/20|    A-05-22|\n",
            "|          Brooklyn|       McCarren Park|3/1/20|    D-22-32|\n",
            "|  Uptown Manhattan|   St. Nicholas Park|3/1/20|    A-04-01|\n",
            "| Midtown Manhattan|Tompkins Square Park|3/1/20|    B-11-19|\n",
            "|  Uptown Manhattan|Riverside Park (s...|3/1/20|    A-05-18|\n",
            "| Midtown Manhattan|   Union Square Park|3/1/20|    B-08-06|\n",
            "|          Brooklyn|       McCarren Park|3/1/20|    D-22-03|\n",
            "+------------------+--------------------+------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Broadcast join\n",
        "# identify the tables candidates for broadcast (smaller one)\n",
        "\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "join_df = (squirrel\n",
        "           .join(park, on=\"Park ID\", how=\"inner\")\n",
        "           .join(broadcast(area), on=\"Area ID\", how=\"inner\")\n",
        "           .select(area[\"Area Description\"], park[\"Park Name\"], park[\"Date\"], squirrel[\"Squirrel ID\"])\n",
        "           )\n",
        "\n",
        "join_df.explain()\n",
        "join_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Repartition & Coalesce\n",
        "\n",
        "- coalesce is for reducing partitions without shuffling\n",
        "- repartition is for distributing data evenly across the cluster for better parallelism\n",
        "\n",
        "- if possible choose coalesce over repartition\n",
        "- if needed to increase partitions to increase parallelism, use repartition, however keep the data shuffling operation in mind\n",
        "\n"
      ],
      "metadata": {
        "id": "K5wkJmgQs8UX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "squirrel_1 = squirrel\n",
        "squirrel_2 = squirrel\n",
        "\n",
        "# Check partitions\n",
        "squirrel_1.rdd.getNumPartitions()\n",
        "\n",
        "# RDD -> partitions among the workers"
      ],
      "metadata": {
        "id": "EitF3d_es8Fo",
        "outputId": "2cc0f909-85a2-4bca-ceed-4e6ceb579509",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# repartition\n",
        "# evenly distribute date across partitions for better parallel processing efficiency\n",
        "# increase AND reduce partitions\n",
        "# do shuffling\n",
        "\n",
        "print(f\"before repartition: {squirrel_1.rdd.getNumPartitions()}\")\n",
        "squirrel_1 = squirrel_1.repartition(4)\n",
        "print(f\"after repartition: {squirrel_1.rdd.getNumPartitions()}\")"
      ],
      "metadata": {
        "id": "wKBD2qXCtzwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4070b321-b8f9-4555-dcdd-9b202c5a8c13"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before repartition: 1\n",
            "after repartition: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# coalesce\n",
        "# reduce partitions without shuffling\n",
        "# minimizes data movement across the cluster\n",
        "\n",
        "# does not allow to increase partitions, only reduce\n",
        "print(f\"before coalesce: {squirrel_2.rdd.getNumPartitions()}\")\n",
        "squirrel_2 = squirrel_2.coalesce(5)\n",
        "print(f\"after coalesce: {squirrel_2.rdd.getNumPartitions()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72Mk2UJwLEKv",
        "outputId": "65b64e80-d116-4e2a-c8c9-981b76112554"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before coalesce: 1\n",
            "after coalesce: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"before coalesce: {squirrel_1.rdd.getNumPartitions()}\")\n",
        "squirrel_1 = squirrel_1.coalesce(2)\n",
        "print(f\"after coalesce: {squirrel_1.rdd.getNumPartitions()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ER9DqONMKE2",
        "outputId": "d51bc971-5f77-4cf9-a145-66f1feb1af51"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before coalesce: 4\n",
            "after coalesce: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# repartition/coalesce and writing data\n",
        "!rm -rf /content/files/area\n",
        "!mkdir -p /content/files/area\n",
        "\n",
        "# repartition \"area\" dataframe and write as parquet\n",
        "area.repartition(3).write.format(\"parquet\").mode(\"overwrite\").save(\"/content/files/area\")"
      ],
      "metadata": {
        "id": "IR5COTOyPT_t"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check files and their content\n",
        "\n",
        "files = [\"part-00000-ada7e48d-09f0-4f16-96bf-2e59e26dbb39-c000.snappy.parquet\",\n",
        "         \"part-00001-ada7e48d-09f0-4f16-96bf-2e59e26dbb39-c000.snappy.parquet\",\n",
        "         \"part-00002-ada7e48d-09f0-4f16-96bf-2e59e26dbb39-c000.snappy.parquet\"]\n",
        "folder = \"/content/files/area/\"\n",
        "\n",
        "for f in files:\n",
        "  df = spark.read.parquet(f\"{folder}{f}\")\n",
        "  print(f\"{f} - {df.count()} rows\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bl3NMEx6PhYV",
        "outputId": "69927d29-6b47-41f1-ed11-4c8a1e351dcb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000-ada7e48d-09f0-4f16-96bf-2e59e26dbb39-c000.snappy.parquet - 2 rows\n",
            "part-00001-ada7e48d-09f0-4f16-96bf-2e59e26dbb39-c000.snappy.parquet - 1 rows\n",
            "part-00002-ada7e48d-09f0-4f16-96bf-2e59e26dbb39-c000.snappy.parquet - 1 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check file sizes\n",
        "!ls -lah /content/files/area"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5H-rpwqQS_M",
        "outputId": "d7d3525b-0482-49ab-fb3a-d2d7a2b01c00"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 36K\n",
            "drwxr-xr-x 2 root root 4.0K Nov 16 15:28 .\n",
            "drwxr-xr-x 3 root root 4.0K Nov 16 15:28 ..\n",
            "-rw-r--r-- 1 root root 1.7K Nov 16 15:28 part-00000-ada7e48d-09f0-4f16-96bf-2e59e26dbb39-c000.snappy.parquet\n",
            "-rw-r--r-- 1 root root   24 Nov 16 15:28 .part-00000-ada7e48d-09f0-4f16-96bf-2e59e26dbb39-c000.snappy.parquet.crc\n",
            "-rw-r--r-- 1 root root 1.7K Nov 16 15:28 part-00001-ada7e48d-09f0-4f16-96bf-2e59e26dbb39-c000.snappy.parquet\n",
            "-rw-r--r-- 1 root root   24 Nov 16 15:28 .part-00001-ada7e48d-09f0-4f16-96bf-2e59e26dbb39-c000.snappy.parquet.crc\n",
            "-rw-r--r-- 1 root root 1.7K Nov 16 15:28 part-00002-ada7e48d-09f0-4f16-96bf-2e59e26dbb39-c000.snappy.parquet\n",
            "-rw-r--r-- 1 root root   24 Nov 16 15:28 .part-00002-ada7e48d-09f0-4f16-96bf-2e59e26dbb39-c000.snappy.parquet.crc\n",
            "-rw-r--r-- 1 root root    0 Nov 16 15:28 _SUCCESS\n",
            "-rw-r--r-- 1 root root    8 Nov 16 15:28 ._SUCCESS.crc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question"
      ],
      "metadata": {
        "id": "aVyWuRHiR--p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1\n",
        "# read data from /content/files/area (3 parquet files)\n",
        "# write again the data into the same folder making sure the output will be only one file"
      ],
      "metadata": {
        "id": "hLjzGtv-Rou1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Ler todos os arquivos Parquet na pasta\n",
        "df = spark.read.format(\"parquet\").load(\"/content/files/area\")\n",
        "\n",
        "#df.rdd.getNumPartitions()\n",
        "df.write.format(\"parquet\").mode(\"overwrite\").save(\"/content/files/area\")"
      ],
      "metadata": {
        "id": "FRNdMu83mU_W",
        "outputId": "e64e52cc-6d0d-4b7e-ce29-d7d2c15996cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o285.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 90.0 failed 1 times, most recent failure: Lost task 0.0 in stage 90.0 (TID 2255) (b062ed0df34a executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/content/files/area.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/content/files/area/part-00002-ada7e48d-09f0-4f16-96bf-2e59e26dbb39-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n\t... 17 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/content/files/area.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/content/files/area/part-00002-ada7e48d-09f0-4f16-96bf-2e59e26dbb39-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n\t... 17 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-c685b050cafa>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#df.rdd.getNumPartitions()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/files/area\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1461\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minsertInto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o285.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 90.0 failed 1 times, most recent failure: Lost task 0.0 in stage 90.0 (TID 2255) (b062ed0df34a executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/content/files/area.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/content/files/area/part-00002-ada7e48d-09f0-4f16-96bf-2e59e26dbb39-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n\t... 17 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to file:/content/files/area.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.spark.SparkFileNotFoundException: File file:/content/files/area/part-00002-ada7e48d-09f0-4f16-96bf-2e59e26dbb39-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.readCurrentFileNotFoundError(QueryExecutionErrors.scala:781)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:220)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n\t... 17 more\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}